Spark assembly has been built with Hive, including Datanucleus jars on classpath
2015-02-17 16:27:25,007 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-17 16:27:25,804 INFO  workflow.CreateWorkflow$ - Extracting datasource params...
2015-02-17 16:27:25,877 INFO  workflow.CreateWorkflow$ - No 'name' is found. Default empty String wil be used.
2015-02-17 16:27:25,889 INFO  workflow.CreateWorkflow$ - datasource: (,DataSourceParams(9))
2015-02-17 16:27:25,889 INFO  workflow.CreateWorkflow$ - Extracting preparator params...
2015-02-17 16:27:25,892 INFO  workflow.CreateWorkflow$ - preparator: (,Empty)
2015-02-17 16:27:25,897 INFO  workflow.CreateWorkflow$ - Extracting serving params...
2015-02-17 16:27:25,898 INFO  workflow.CreateWorkflow$ - serving: (,Empty)
2015-02-17 16:27:26,746 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run
2015-02-17 16:27:26,747 INFO  workflow.CoreWorkflow$ - Start spark context
2015-02-17 16:27:34,269 INFO  workflow.CoreWorkflow$ - Data sanity checking is on.
2015-02-17 16:27:34,270 INFO  workflow.CoreWorkflow$ - Data Source
Gathering data from event server.
2015-02-17 16:27:40,181 INFO  workflow.CoreWorkflow$ - Number of training set: 1
2015-02-17 16:27:40,182 INFO  workflow.CoreWorkflow$ - Performing data sanity check on training data.
2015-02-17 16:27:40,183 INFO  workflow.CoreWorkflow$ - org.template.vanilla.TrainingData does not support data sanity check. Skipping check.
2015-02-17 16:27:40,184 INFO  workflow.CoreWorkflow$ - Data source complete
2015-02-17 16:27:40,184 INFO  workflow.CoreWorkflow$ - Preparator
2015-02-17 16:27:40,185 INFO  workflow.CoreWorkflow$ - Performing data sanity check on prepared data.
2015-02-17 16:27:40,186 INFO  workflow.CoreWorkflow$ - org.template.vanilla.PreparedData does not support data sanity check. Skipping check.
2015-02-17 16:27:40,187 INFO  workflow.CoreWorkflow$ - Preparator complete
2015-02-17 16:27:40,187 INFO  workflow.CoreWorkflow$ - Algo model construction
Running the K-Means clustering algorithm.
2015-02-17 16:27:55,241 ERROR executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.hadoop.hbase.DoNotRetryIOException: Failed after retry of OutOfOrderScannerNextException: was there a rpc timeout?
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:403)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:232)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReader.nextKeyValue(TableRecordReader.java:138)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:145)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:58)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 1 But the nextCallSeq got from client: 0; request=scanner_id: 22 number_of_rows: 500 close_scanner: false next_call_seq: 0
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3195)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29941)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2029)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:108)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:112)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:92)
	at java.lang.Thread.run(Thread.java:745)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:97)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:214)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:59)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:355)
	... 15 more
2015-02-17 16:27:55,261 ERROR scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.hadoop.hbase.DoNotRetryIOException: Failed after retry of OutOfOrderScannerNextException: was there a rpc timeout?
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:403)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:232)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReader.nextKeyValue(TableRecordReader.java:138)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:145)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:58)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 1 But the nextCallSeq got from client: 0; request=scanner_id: 22 number_of_rows: 500 close_scanner: false next_call_seq: 0
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3195)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29941)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2029)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:108)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:112)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:92)
	at java.lang.Thread.run(Thread.java:745)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:97)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:214)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:59)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:355)
	... 15 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
sahil@Sahil:~/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified$ pio build; pio train;
2015-02-17 16:29:29,799 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:29:29,801 INFO  tools.Console$ - Development tree detected. Building built-in engines.
2015-02-17 16:29:29,803 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at /home/sahil/PredictionIO to build.
2015-02-17 16:29:29,804 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:29:29,810 INFO  tools.Console$ - Going to run: [/home/sahil/PredictionIO/sbt/sbt, engines/publishLocal, engines/assemblyPackageDependency]
2015-02-17 16:29:44,546 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:29:44,547 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at the current working directory to build.
2015-02-17 16:29:44,548 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:29:44,550 INFO  tools.Console$ - Uber JAR disabled. Making sure lib/pio-assembly-0.8.5.jar is absent.
2015-02-17 16:29:44,551 INFO  tools.Console$ - Going to run: /home/sahil/PredictionIO/sbt/sbt  package assemblyPackageDependency
2015-02-17 16:29:51,105 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:29:51,106 INFO  tools.Console$ - Looking for an engine...
2015-02-17 16:29:51,121 INFO  tools.Console$ - Found template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:29:51,121 INFO  tools.Console$ - Found template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:29:51,134 INFO  tools.Console$ - HADOOP_CONF_DIR is not set. Assuming HDFS is unavailable.
2015-02-17 16:29:52,657 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:29:52,675 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:29:52,776 INFO  tools.RegisterEngine$ - Registering engine dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91
2015-02-17 16:29:52,794 INFO  tools.Console$ - Your engine is ready for training.
2015-02-17 16:29:53,777 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:29:55,308 INFO  tools.RunWorkflow$ - Submission command: /home/sahil/PredictionIO/spark-1.2.0-bin-hadoop2.4/bin/spark-submit --class io.prediction.workflow.CreateWorkflow --name PredictionIO Training: dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 () --jars file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar,file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines_2.10-0.8.5.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines-assembly-0.8.5-deps.jar --files /home/sahil/PredictionIO/conf/hbase-site.xml --driver-class-path /home/sahil/PredictionIO/conf /home/sahil/PredictionIO/assembly/pio-assembly-0.8.5.jar --env PIO_STORAGE_SOURCES_HBASE_TYPE=hbase,PIO_ENV_LOADED=1,PIO_STORAGE_SOURCES_HBASE_HOSTS=0,PIO_STORAGE_REPOSITORIES_METADATA_NAME=predictionio_metadata,PIO_FS_BASEDIR=/home/sahil/.pio_store,PIO_STORAGE_SOURCES_ELASTICSEARCH_HOSTS=localhost,PIO_HOME=/home/sahil/PredictionIO,PIO_FS_ENGINESDIR=/home/sahil/.pio_store/engines,PIO_STORAGE_SOURCES_HBASE_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_TYPE=elasticsearch,PIO_STORAGE_REPOSITORIES_METADATA_SOURCE=ELASTICSEARCH,PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=LOCALFS,PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=predictionio_eventdata,PIO_FS_TMPDIR=/home/sahil/.pio_store/tmp,PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_,PIO_STORAGE_SOURCES_LOCALFS_HOSTS=/home/sahil/.pio_store/models,PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE=HBASE,PIO_STORAGE_SOURCES_LOCALFS_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_PORTS=9300,PIO_STORAGE_SOURCES_LOCALFS_TYPE=localfs --engineId dem8XD7VweLWk5XAVX0as3J7APzTmnWw --engineVersion 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 --engineVariant /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/engine.json --verbosity 0 --jsonBasePath params
Spark assembly has been built with Hive, including Datanucleus jars on classpath
2015-02-17 16:29:56,568 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-17 16:29:57,362 INFO  workflow.CreateWorkflow$ - Extracting datasource params...
2015-02-17 16:29:57,433 INFO  workflow.CreateWorkflow$ - No 'name' is found. Default empty String wil be used.
2015-02-17 16:29:57,444 INFO  workflow.CreateWorkflow$ - datasource: (,DataSourceParams(9))
2015-02-17 16:29:57,445 INFO  workflow.CreateWorkflow$ - Extracting preparator params...
2015-02-17 16:29:57,447 INFO  workflow.CreateWorkflow$ - preparator: (,Empty)
2015-02-17 16:29:57,452 INFO  workflow.CreateWorkflow$ - Extracting serving params...
2015-02-17 16:29:57,454 INFO  workflow.CreateWorkflow$ - serving: (,Empty)
2015-02-17 16:29:58,286 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run
2015-02-17 16:29:58,287 INFO  workflow.CoreWorkflow$ - Start spark context
2015-02-17 16:30:05,874 INFO  workflow.CoreWorkflow$ - Data sanity checking is on.
2015-02-17 16:30:05,874 INFO  workflow.CoreWorkflow$ - Data Source
Gathering data from event server.
2015-02-17 16:30:12,275 INFO  workflow.CoreWorkflow$ - Number of training set: 1
2015-02-17 16:30:12,276 INFO  workflow.CoreWorkflow$ - Performing data sanity check on training data.
2015-02-17 16:30:12,277 INFO  workflow.CoreWorkflow$ - org.template.vanilla.TrainingData does not support data sanity check. Skipping check.
2015-02-17 16:30:12,277 INFO  workflow.CoreWorkflow$ - Data source complete
2015-02-17 16:30:12,278 INFO  workflow.CoreWorkflow$ - Preparator
2015-02-17 16:30:12,279 INFO  workflow.CoreWorkflow$ - Performing data sanity check on prepared data.
2015-02-17 16:30:12,279 INFO  workflow.CoreWorkflow$ - org.template.vanilla.PreparedData does not support data sanity check. Skipping check.
2015-02-17 16:30:12,280 INFO  workflow.CoreWorkflow$ - Preparator complete
2015-02-17 16:30:12,280 INFO  workflow.CoreWorkflow$ - Algo model construction
Running the K-Means clustering algorithm.
2015-02-17 16:30:30,746 INFO  workflow.CoreWorkflow$ - Performing data sanity check on model data.
2015-02-17 16:30:30,749 INFO  workflow.CoreWorkflow$ - org.apache.spark.rdd.ParallelCollectionRDD does not support data sanity check. Skipping check.
2015-02-17 16:30:30,750 INFO  workflow.CoreWorkflow$ - Evaluator is null. Stop here
2015-02-17 16:30:31,155 INFO  workflow.CoreWorkflow$ - Saved engine instance with ID: h7BaP5__R2Gp5NjLawm2GQ
2015-02-17 16:30:31,156 INFO  workflow.CoreWorkflow$ - Stop spark context
2015-02-17 16:30:32,366 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run completed.
2015-02-17 16:30:32,367 INFO  workflow.CoreWorkflow$ - Your engine has been trained successfully.
sahil@Sahil:~/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified$ 
sahil@Sahil:~/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified$ pio build; pio train;2015-02-17 16:48:44,402 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:48:44,404 INFO  tools.Console$ - Development tree detected. Building built-in engines.
2015-02-17 16:48:44,407 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at /home/sahil/PredictionIO to build.
2015-02-17 16:48:44,407 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:48:44,414 INFO  tools.Console$ - Going to run: [/home/sahil/PredictionIO/sbt/sbt, engines/publishLocal, engines/assemblyPackageDependency]
2015-02-17 16:48:59,620 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:48:59,622 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at the current working directory to build.
2015-02-17 16:48:59,622 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:48:59,624 INFO  tools.Console$ - Uber JAR disabled. Making sure lib/pio-assembly-0.8.5.jar is absent.
2015-02-17 16:48:59,625 INFO  tools.Console$ - Going to run: /home/sahil/PredictionIO/sbt/sbt  package assemblyPackageDependency
2015-02-17 16:49:06,216 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:49:06,217 INFO  tools.Console$ - Looking for an engine...
2015-02-17 16:49:06,232 INFO  tools.Console$ - Found template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:49:06,232 INFO  tools.Console$ - Found template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:49:06,244 INFO  tools.Console$ - HADOOP_CONF_DIR is not set. Assuming HDFS is unavailable.
2015-02-17 16:49:07,837 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:49:07,898 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:49:07,974 INFO  tools.RegisterEngine$ - Registering engine dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91
2015-02-17 16:49:07,991 INFO  tools.Console$ - Your engine is ready for training.
2015-02-17 16:49:08,956 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:49:10,554 INFO  tools.RunWorkflow$ - Submission command: /home/sahil/PredictionIO/spark-1.2.0-bin-hadoop2.4/bin/spark-submit --class io.prediction.workflow.CreateWorkflow --name PredictionIO Training: dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 () --jars file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar,file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines_2.10-0.8.5.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines-assembly-0.8.5-deps.jar --files /home/sahil/PredictionIO/conf/hbase-site.xml --driver-class-path /home/sahil/PredictionIO/conf /home/sahil/PredictionIO/assembly/pio-assembly-0.8.5.jar --env PIO_STORAGE_SOURCES_HBASE_TYPE=hbase,PIO_ENV_LOADED=1,PIO_STORAGE_SOURCES_HBASE_HOSTS=0,PIO_STORAGE_REPOSITORIES_METADATA_NAME=predictionio_metadata,PIO_FS_BASEDIR=/home/sahil/.pio_store,PIO_STORAGE_SOURCES_ELASTICSEARCH_HOSTS=localhost,PIO_HOME=/home/sahil/PredictionIO,PIO_FS_ENGINESDIR=/home/sahil/.pio_store/engines,PIO_STORAGE_SOURCES_HBASE_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_TYPE=elasticsearch,PIO_STORAGE_REPOSITORIES_METADATA_SOURCE=ELASTICSEARCH,PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=LOCALFS,PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=predictionio_eventdata,PIO_FS_TMPDIR=/home/sahil/.pio_store/tmp,PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_,PIO_STORAGE_SOURCES_LOCALFS_HOSTS=/home/sahil/.pio_store/models,PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE=HBASE,PIO_STORAGE_SOURCES_LOCALFS_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_PORTS=9300,PIO_STORAGE_SOURCES_LOCALFS_TYPE=localfs --engineId dem8XD7VweLWk5XAVX0as3J7APzTmnWw --engineVersion 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 --engineVariant /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/engine.json --verbosity 0 --jsonBasePath params
Spark assembly has been built with Hive, including Datanucleus jars on classpath
2015-02-17 16:49:11,860 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-17 16:49:12,663 INFO  workflow.CreateWorkflow$ - Extracting datasource params...
2015-02-17 16:49:12,735 INFO  workflow.CreateWorkflow$ - No 'name' is found. Default empty String wil be used.
2015-02-17 16:49:12,747 INFO  workflow.CreateWorkflow$ - datasource: (,DataSourceParams(9))
2015-02-17 16:49:12,747 INFO  workflow.CreateWorkflow$ - Extracting preparator params...
2015-02-17 16:49:12,749 INFO  workflow.CreateWorkflow$ - preparator: (,Empty)
2015-02-17 16:49:12,755 INFO  workflow.CreateWorkflow$ - Extracting serving params...
2015-02-17 16:49:12,756 INFO  workflow.CreateWorkflow$ - serving: (,Empty)
2015-02-17 16:49:13,605 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run
2015-02-17 16:49:13,606 INFO  workflow.CoreWorkflow$ - Start spark context
2015-02-17 16:49:24,803 INFO  workflow.CoreWorkflow$ - Data sanity checking is on.
2015-02-17 16:49:24,803 INFO  workflow.CoreWorkflow$ - Data Source
Gathering data from event server.
2015-02-17 16:49:30,134 INFO  workflow.CoreWorkflow$ - Number of training set: 1
2015-02-17 16:49:30,135 INFO  workflow.CoreWorkflow$ - Performing data sanity check on training data.
2015-02-17 16:49:30,136 INFO  workflow.CoreWorkflow$ - org.template.vanilla.TrainingData does not support data sanity check. Skipping check.
2015-02-17 16:49:30,136 INFO  workflow.CoreWorkflow$ - Data source complete
2015-02-17 16:49:30,136 INFO  workflow.CoreWorkflow$ - Preparator
2015-02-17 16:49:30,137 INFO  workflow.CoreWorkflow$ - Performing data sanity check on prepared data.
2015-02-17 16:49:30,138 INFO  workflow.CoreWorkflow$ - org.template.vanilla.PreparedData does not support data sanity check. Skipping check.
2015-02-17 16:49:30,138 INFO  workflow.CoreWorkflow$ - Preparator complete
2015-02-17 16:49:30,138 INFO  workflow.CoreWorkflow$ - Algo model construction
Running the K-Means clustering algorithm.
2015-02-17 16:49:46,295 INFO  workflow.CoreWorkflow$ - Performing data sanity check on model data.
2015-02-17 16:49:46,297 INFO  workflow.CoreWorkflow$ - org.apache.spark.rdd.ParallelCollectionRDD does not support data sanity check. Skipping check.
2015-02-17 16:49:46,298 INFO  workflow.CoreWorkflow$ - Evaluator is null. Stop here
2015-02-17 16:49:46,687 INFO  workflow.CoreWorkflow$ - Saved engine instance with ID: mKwLA7DARNG5OEXC9IGb9Q
2015-02-17 16:49:46,688 INFO  workflow.CoreWorkflow$ - Stop spark context
2015-02-17 16:49:47,875 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run completed.
2015-02-17 16:49:47,875 INFO  workflow.CoreWorkflow$ - Your engine has been trained successfully.
Spark assembly has been built with Hive, including Datanucleus jars on classpath
2015-02-17 16:27:25,007 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-17 16:27:25,804 INFO  workflow.CreateWorkflow$ - Extracting datasource params...
2015-02-17 16:27:25,877 INFO  workflow.CreateWorkflow$ - No 'name' is found. Default empty String wil be used.
2015-02-17 16:27:25,889 INFO  workflow.CreateWorkflow$ - datasource: (,DataSourceParams(9))
2015-02-17 16:27:25,889 INFO  workflow.CreateWorkflow$ - Extracting preparator params...
2015-02-17 16:27:25,892 INFO  workflow.CreateWorkflow$ - preparator: (,Empty)
2015-02-17 16:27:25,897 INFO  workflow.CreateWorkflow$ - Extracting serving params...
2015-02-17 16:27:25,898 INFO  workflow.CreateWorkflow$ - serving: (,Empty)
2015-02-17 16:27:26,746 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run
2015-02-17 16:27:26,747 INFO  workflow.CoreWorkflow$ - Start spark context
2015-02-17 16:27:34,269 INFO  workflow.CoreWorkflow$ - Data sanity checking is on.
2015-02-17 16:27:34,270 INFO  workflow.CoreWorkflow$ - Data Source
Gathering data from event server.
2015-02-17 16:27:40,181 INFO  workflow.CoreWorkflow$ - Number of training set: 1
2015-02-17 16:27:40,182 INFO  workflow.CoreWorkflow$ - Performing data sanity check on training data.
2015-02-17 16:27:40,183 INFO  workflow.CoreWorkflow$ - org.template.vanilla.TrainingData does not support data sanity check. Skipping check.
2015-02-17 16:27:40,184 INFO  workflow.CoreWorkflow$ - Data source complete
2015-02-17 16:27:40,184 INFO  workflow.CoreWorkflow$ - Preparator
2015-02-17 16:27:40,185 INFO  workflow.CoreWorkflow$ - Performing data sanity check on prepared data.
2015-02-17 16:27:40,186 INFO  workflow.CoreWorkflow$ - org.template.vanilla.PreparedData does not support data sanity check. Skipping check.
2015-02-17 16:27:40,187 INFO  workflow.CoreWorkflow$ - Preparator complete
2015-02-17 16:27:40,187 INFO  workflow.CoreWorkflow$ - Algo model construction
Running the K-Means clustering algorithm.
2015-02-17 16:27:55,241 ERROR executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.hadoop.hbase.DoNotRetryIOException: Failed after retry of OutOfOrderScannerNextException: was there a rpc timeout?
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:403)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:232)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReader.nextKeyValue(TableRecordReader.java:138)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:145)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:58)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 1 But the nextCallSeq got from client: 0; request=scanner_id: 22 number_of_rows: 500 close_scanner: false next_call_seq: 0
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3195)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29941)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2029)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:108)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:112)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:92)
	at java.lang.Thread.run(Thread.java:745)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:97)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:214)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:59)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:355)
	... 15 more
2015-02-17 16:27:55,261 ERROR scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.hadoop.hbase.DoNotRetryIOException: Failed after retry of OutOfOrderScannerNextException: was there a rpc timeout?
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:403)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:232)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReader.nextKeyValue(TableRecordReader.java:138)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:145)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:58)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 1 But the nextCallSeq got from client: 0; request=scanner_id: 22 number_of_rows: 500 close_scanner: false next_call_seq: 0
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3195)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29941)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2029)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:108)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:112)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:92)
	at java.lang.Thread.run(Thread.java:745)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:97)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:214)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:59)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:355)
	... 15 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
sahil@Sahil:~/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified$ pio build; pio train;
2015-02-17 16:29:29,799 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:29:29,801 INFO  tools.Console$ - Development tree detected. Building built-in engines.
2015-02-17 16:29:29,803 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at /home/sahil/PredictionIO to build.
2015-02-17 16:29:29,804 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:29:29,810 INFO  tools.Console$ - Going to run: [/home/sahil/PredictionIO/sbt/sbt, engines/publishLocal, engines/assemblyPackageDependency]
2015-02-17 16:29:44,546 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:29:44,547 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at the current working directory to build.
2015-02-17 16:29:44,548 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:29:44,550 INFO  tools.Console$ - Uber JAR disabled. Making sure lib/pio-assembly-0.8.5.jar is absent.
2015-02-17 16:29:44,551 INFO  tools.Console$ - Going to run: /home/sahil/PredictionIO/sbt/sbt  package assemblyPackageDependency
2015-02-17 16:29:51,105 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:29:51,106 INFO  tools.Console$ - Looking for an engine...
2015-02-17 16:29:51,121 INFO  tools.Console$ - Found template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:29:51,121 INFO  tools.Console$ - Found template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:29:51,134 INFO  tools.Console$ - HADOOP_CONF_DIR is not set. Assuming HDFS is unavailable.
2015-02-17 16:29:52,657 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:29:52,675 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:29:52,776 INFO  tools.RegisterEngine$ - Registering engine dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91
2015-02-17 16:29:52,794 INFO  tools.Console$ - Your engine is ready for training.
2015-02-17 16:29:53,777 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:29:55,308 INFO  tools.RunWorkflow$ - Submission command: /home/sahil/PredictionIO/spark-1.2.0-bin-hadoop2.4/bin/spark-submit --class io.prediction.workflow.CreateWorkflow --name PredictionIO Training: dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 () --jars file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar,file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines_2.10-0.8.5.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines-assembly-0.8.5-deps.jar --files /home/sahil/PredictionIO/conf/hbase-site.xml --driver-class-path /home/sahil/PredictionIO/conf /home/sahil/PredictionIO/assembly/pio-assembly-0.8.5.jar --env PIO_STORAGE_SOURCES_HBASE_TYPE=hbase,PIO_ENV_LOADED=1,PIO_STORAGE_SOURCES_HBASE_HOSTS=0,PIO_STORAGE_REPOSITORIES_METADATA_NAME=predictionio_metadata,PIO_FS_BASEDIR=/home/sahil/.pio_store,PIO_STORAGE_SOURCES_ELASTICSEARCH_HOSTS=localhost,PIO_HOME=/home/sahil/PredictionIO,PIO_FS_ENGINESDIR=/home/sahil/.pio_store/engines,PIO_STORAGE_SOURCES_HBASE_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_TYPE=elasticsearch,PIO_STORAGE_REPOSITORIES_METADATA_SOURCE=ELASTICSEARCH,PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=LOCALFS,PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=predictionio_eventdata,PIO_FS_TMPDIR=/home/sahil/.pio_store/tmp,PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_,PIO_STORAGE_SOURCES_LOCALFS_HOSTS=/home/sahil/.pio_store/models,PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE=HBASE,PIO_STORAGE_SOURCES_LOCALFS_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_PORTS=9300,PIO_STORAGE_SOURCES_LOCALFS_TYPE=localfs --engineId dem8XD7VweLWk5XAVX0as3J7APzTmnWw --engineVersion 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 --engineVariant /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/engine.json --verbosity 0 --jsonBasePath params
Spark assembly has been built with Hive, including Datanucleus jars on classpath
2015-02-17 16:29:56,568 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-17 16:29:57,362 INFO  workflow.CreateWorkflow$ - Extracting datasource params...
2015-02-17 16:29:57,433 INFO  workflow.CreateWorkflow$ - No 'name' is found. Default empty String wil be used.
2015-02-17 16:29:57,444 INFO  workflow.CreateWorkflow$ - datasource: (,DataSourceParams(9))
2015-02-17 16:29:57,445 INFO  workflow.CreateWorkflow$ - Extracting preparator params...
2015-02-17 16:29:57,447 INFO  workflow.CreateWorkflow$ - preparator: (,Empty)
2015-02-17 16:29:57,452 INFO  workflow.CreateWorkflow$ - Extracting serving params...
2015-02-17 16:29:57,454 INFO  workflow.CreateWorkflow$ - serving: (,Empty)
2015-02-17 16:29:58,286 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run
2015-02-17 16:29:58,287 INFO  workflow.CoreWorkflow$ - Start spark context
2015-02-17 16:30:05,874 INFO  workflow.CoreWorkflow$ - Data sanity checking is on.
2015-02-17 16:30:05,874 INFO  workflow.CoreWorkflow$ - Data Source
Gathering data from event server.
2015-02-17 16:30:12,275 INFO  workflow.CoreWorkflow$ - Number of training set: 1
2015-02-17 16:30:12,276 INFO  workflow.CoreWorkflow$ - Performing data sanity check on training data.
2015-02-17 16:30:12,277 INFO  workflow.CoreWorkflow$ - org.template.vanilla.TrainingData does not support data sanity check. Skipping check.
2015-02-17 16:30:12,277 INFO  workflow.CoreWorkflow$ - Data source complete
2015-02-17 16:30:12,278 INFO  workflow.CoreWorkflow$ - Preparator
2015-02-17 16:30:12,279 INFO  workflow.CoreWorkflow$ - Performing data sanity check on prepared data.
2015-02-17 16:30:12,279 INFO  workflow.CoreWorkflow$ - org.template.vanilla.PreparedData does not support data sanity check. Skipping check.
2015-02-17 16:30:12,280 INFO  workflow.CoreWorkflow$ - Preparator complete
2015-02-17 16:30:12,280 INFO  workflow.CoreWorkflow$ - Algo model construction
Running the K-Means clustering algorithm.
2015-02-17 16:30:30,746 INFO  workflow.CoreWorkflow$ - Performing data sanity check on model data.
2015-02-17 16:30:30,749 INFO  workflow.CoreWorkflow$ - org.apache.spark.rdd.ParallelCollectionRDD does not support data sanity check. Skipping check.
2015-02-17 16:30:30,750 INFO  workflow.CoreWorkflow$ - Evaluator is null. Stop here
2015-02-17 16:30:31,155 INFO  workflow.CoreWorkflow$ - Saved engine instance with ID: h7BaP5__R2Gp5NjLawm2GQ
2015-02-17 16:30:31,156 INFO  workflow.CoreWorkflow$ - Stop spark context
2015-02-17 16:30:32,366 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run completed.
2015-02-17 16:30:32,367 INFO  workflow.CoreWorkflow$ - Your engine has been trained successfully.
sahil@Sahil:~/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified$ 
sahil@Sahil:~/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified$ pio build; pio train;2015-02-17 16:48:44,402 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:48:44,404 INFO  tools.Console$ - Development tree detected. Building built-in engines.
2015-02-17 16:48:44,407 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at /home/sahil/PredictionIO to build.
2015-02-17 16:48:44,407 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:48:44,414 INFO  tools.Console$ - Going to run: [/home/sahil/PredictionIO/sbt/sbt, engines/publishLocal, engines/assemblyPackageDependency]
2015-02-17 16:48:59,620 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:48:59,622 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at the current working directory to build.
2015-02-17 16:48:59,622 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:48:59,624 INFO  tools.Console$ - Uber JAR disabled. Making sure lib/pio-assembly-0.8.5.jar is absent.
2015-02-17 16:48:59,625 INFO  tools.Console$ - Going to run: /home/sahil/PredictionIO/sbt/sbt  package assemblyPackageDependency
2015-02-17 16:49:06,216 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:49:06,217 INFO  tools.Console$ - Looking for an engine...
2015-02-17 16:49:06,232 INFO  tools.Console$ - Found template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:49:06,232 INFO  tools.Console$ - Found template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:49:06,244 INFO  tools.Console$ - HADOOP_CONF_DIR is not set. Assuming HDFS is unavailable.
2015-02-17 16:49:07,837 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:49:07,898 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:49:07,974 INFO  tools.RegisterEngine$ - Registering engine dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91
2015-02-17 16:49:07,991 INFO  tools.Console$ - Your engine is ready for training.
2015-02-17 16:49:08,956 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:49:10,554 INFO  tools.RunWorkflow$ - Submission command: /home/sahil/PredictionIO/spark-1.2.0-bin-hadoop2.4/bin/spark-submit --class io.prediction.workflow.CreateWorkflow --name PredictionIO Training: dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 () --jars file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar,file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines_2.10-0.8.5.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines-assembly-0.8.5-deps.jar --files /home/sahil/PredictionIO/conf/hbase-site.xml --driver-class-path /home/sahil/PredictionIO/conf /home/sahil/PredictionIO/assembly/pio-assembly-0.8.5.jar --env PIO_STORAGE_SOURCES_HBASE_TYPE=hbase,PIO_ENV_LOADED=1,PIO_STORAGE_SOURCES_HBASE_HOSTS=0,PIO_STORAGE_REPOSITORIES_METADATA_NAME=predictionio_metadata,PIO_FS_BASEDIR=/home/sahil/.pio_store,PIO_STORAGE_SOURCES_ELASTICSEARCH_HOSTS=localhost,PIO_HOME=/home/sahil/PredictionIO,PIO_FS_ENGINESDIR=/home/sahil/.pio_store/engines,PIO_STORAGE_SOURCES_HBASE_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_TYPE=elasticsearch,PIO_STORAGE_REPOSITORIES_METADATA_SOURCE=ELASTICSEARCH,PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=LOCALFS,PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=predictionio_eventdata,PIO_FS_TMPDIR=/home/sahil/.pio_store/tmp,PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_,PIO_STORAGE_SOURCES_LOCALFS_HOSTS=/home/sahil/.pio_store/models,PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE=HBASE,PIO_STORAGE_SOURCES_LOCALFS_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_PORTS=9300,PIO_STORAGE_SOURCES_LOCALFS_TYPE=localfs --engineId dem8XD7VweLWk5XAVX0as3J7APzTmnWw --engineVersion 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 --engineVariant /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/engine.json --verbosity 0 --jsonBasePath params
Spark assembly has been built with Hive, including Datanucleus jars on classpath
2015-02-17 16:49:11,860 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-17 16:49:12,663 INFO  workflow.CreateWorkflow$ - Extracting datasource params...
2015-02-17 16:49:12,735 INFO  workflow.CreateWorkflow$ - No 'name' is found. Default empty String wil be used.
2015-02-17 16:49:12,747 INFO  workflow.CreateWorkflow$ - datasource: (,DataSourceParams(9))
2015-02-17 16:49:12,747 INFO  workflow.CreateWorkflow$ - Extracting preparator params...
2015-02-17 16:49:12,749 INFO  workflow.CreateWorkflow$ - preparator: (,Empty)
2015-02-17 16:49:12,755 INFO  workflow.CreateWorkflow$ - Extracting serving params...
2015-02-17 16:49:12,756 INFO  workflow.CreateWorkflow$ - serving: (,Empty)
2015-02-17 16:49:13,605 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run
2015-02-17 16:49:13,606 INFO  workflow.CoreWorkflow$ - Start spark context
2015-02-17 16:49:24,803 INFO  workflow.CoreWorkflow$ - Data sanity checking is on.
2015-02-17 16:49:24,803 INFO  workflow.CoreWorkflow$ - Data Source
Gathering data from event server.
2015-02-17 16:49:30,134 INFO  workflow.CoreWorkflow$ - Number of training set: 1
2015-02-17 16:49:30,135 INFO  workflow.CoreWorkflow$ - Performing data sanity check on training data.
2015-02-17 16:49:30,136 INFO  workflow.CoreWorkflow$ - org.template.vanilla.TrainingData does not support data sanity check. Skipping check.
2015-02-17 16:49:30,136 INFO  workflow.CoreWorkflow$ - Data source complete
2015-02-17 16:49:30,136 INFO  workflow.CoreWorkflow$ - Preparator
2015-02-17 16:49:30,137 INFO  workflow.CoreWorkflow$ - Performing data sanity check on prepared data.
2015-02-17 16:49:30,138 INFO  workflow.CoreWorkflow$ - org.template.vanilla.PreparedData does not support data sanity check. Skipping check.
2015-02-17 16:49:30,138 INFO  workflow.CoreWorkflow$ - Preparator complete
2015-02-17 16:49:30,138 INFO  workflow.CoreWorkflow$ - Algo model construction
Running the K-Means clustering algorithm.
2015-02-17 16:49:46,295 INFO  workflow.CoreWorkflow$ - Performing data sanity check on model data.
2015-02-17 16:49:46,297 INFO  workflow.CoreWorkflow$ - org.apache.spark.rdd.ParallelCollectionRDD does not support data sanity check. Skipping check.
2015-02-17 16:49:46,298 INFO  workflow.CoreWorkflow$ - Evaluator is null. Stop here
2015-02-17 16:49:46,687 INFO  workflow.CoreWorkflow$ - Saved engine instance with ID: mKwLA7DARNG5OEXC9IGb9Q
2015-02-17 16:49:46,688 INFO  workflow.CoreWorkflow$ - Stop spark context
2015-02-17 16:49:47,875 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run completed.
2015-02-17 16:49:47,875 INFO  workflow.CoreWorkflow$ - Your engine has been trained successfully.
park assembly has been built with Hive, including Datanucleus jars on classpath
2015-02-17 16:27:25,007 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-17 16:27:25,804 INFO  workflow.CreateWorkflow$ - Extracting datasource params...
2015-02-17 16:27:25,877 INFO  workflow.CreateWorkflow$ - No 'name' is found. Default empty String wil be used.
2015-02-17 16:27:25,889 INFO  workflow.CreateWorkflow$ - datasource: (,DataSourceParams(9))
2015-02-17 16:27:25,889 INFO  workflow.CreateWorkflow$ - Extracting preparator params...
2015-02-17 16:27:25,892 INFO  workflow.CreateWorkflow$ - preparator: (,Empty)
2015-02-17 16:27:25,897 INFO  workflow.CreateWorkflow$ - Extracting serving params...
2015-02-17 16:27:25,898 INFO  workflow.CreateWorkflow$ - serving: (,Empty)
2015-02-17 16:27:26,746 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run
2015-02-17 16:27:26,747 INFO  workflow.CoreWorkflow$ - Start spark context
2015-02-17 16:27:34,269 INFO  workflow.CoreWorkflow$ - Data sanity checking is on.
2015-02-17 16:27:34,270 INFO  workflow.CoreWorkflow$ - Data Source
Gathering data from event server.
2015-02-17 16:27:40,181 INFO  workflow.CoreWorkflow$ - Number of training set: 1
2015-02-17 16:27:40,182 INFO  workflow.CoreWorkflow$ - Performing data sanity check on training data.
2015-02-17 16:27:40,183 INFO  workflow.CoreWorkflow$ - org.template.vanilla.TrainingData does not support data sanity check. Skipping check.
2015-02-17 16:27:40,184 INFO  workflow.CoreWorkflow$ - Data source complete
2015-02-17 16:27:40,184 INFO  workflow.CoreWorkflow$ - Preparator
2015-02-17 16:27:40,185 INFO  workflow.CoreWorkflow$ - Performing data sanity check on prepared data.
2015-02-17 16:27:40,186 INFO  workflow.CoreWorkflow$ - org.template.vanilla.PreparedData does not support data sanity check. Skipping check.
2015-02-17 16:27:40,187 INFO  workflow.CoreWorkflow$ - Preparator complete
2015-02-17 16:27:40,187 INFO  workflow.CoreWorkflow$ - Algo model construction
Running the K-Means clustering algorithm.
2015-02-17 16:27:55,241 ERROR executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.hadoop.hbase.DoNotRetryIOException: Failed after retry of OutOfOrderScannerNextException: was there a rpc timeout?
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:403)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:232)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReader.nextKeyValue(TableRecordReader.java:138)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:145)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:58)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 1 But the nextCallSeq got from client: 0; request=scanner_id: 22 number_of_rows: 500 close_scanner: false next_call_seq: 0
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3195)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29941)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2029)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:108)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:112)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:92)
	at java.lang.Thread.run(Thread.java:745)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:97)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:214)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:59)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:355)
	... 15 more
2015-02-17 16:27:55,261 ERROR scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.hadoop.hbase.DoNotRetryIOException: Failed after retry of OutOfOrderScannerNextException: was there a rpc timeout?
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:403)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:232)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReader.nextKeyValue(TableRecordReader.java:138)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:145)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:58)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 1 But the nextCallSeq got from client: 0; request=scanner_id: 22 number_of_rows: 500 close_scanner: false next_call_seq: 0
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3195)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29941)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2029)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:108)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:112)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:92)
	at java.lang.Thread.run(Thread.java:745)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:97)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:214)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:59)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)
	at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:355)
	... 15 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
sahil@Sahil:~/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified$ pio build; pio train;
2015-02-17 16:29:29,799 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:29:29,801 INFO  tools.Console$ - Development tree detected. Building built-in engines.
2015-02-17 16:29:29,803 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at /home/sahil/PredictionIO to build.
2015-02-17 16:29:29,804 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:29:29,810 INFO  tools.Console$ - Going to run: [/home/sahil/PredictionIO/sbt/sbt, engines/publishLocal, engines/assemblyPackageDependency]
2015-02-17 16:29:44,546 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:29:44,547 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at the current working directory to build.
2015-02-17 16:29:44,548 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:29:44,550 INFO  tools.Console$ - Uber JAR disabled. Making sure lib/pio-assembly-0.8.5.jar is absent.
2015-02-17 16:29:44,551 INFO  tools.Console$ - Going to run: /home/sahil/PredictionIO/sbt/sbt  package assemblyPackageDependency
2015-02-17 16:29:51,105 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:29:51,106 INFO  tools.Console$ - Looking for an engine...
2015-02-17 16:29:51,121 INFO  tools.Console$ - Found template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:29:51,121 INFO  tools.Console$ - Found template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:29:51,134 INFO  tools.Console$ - HADOOP_CONF_DIR is not set. Assuming HDFS is unavailable.
2015-02-17 16:29:52,657 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:29:52,675 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:29:52,776 INFO  tools.RegisterEngine$ - Registering engine dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91
2015-02-17 16:29:52,794 INFO  tools.Console$ - Your engine is ready for training.
2015-02-17 16:29:53,777 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:29:55,308 INFO  tools.RunWorkflow$ - Submission command: /home/sahil/PredictionIO/spark-1.2.0-bin-hadoop2.4/bin/spark-submit --class io.prediction.workflow.CreateWorkflow --name PredictionIO Training: dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 () --jars file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar,file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines_2.10-0.8.5.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines-assembly-0.8.5-deps.jar --files /home/sahil/PredictionIO/conf/hbase-site.xml --driver-class-path /home/sahil/PredictionIO/conf /home/sahil/PredictionIO/assembly/pio-assembly-0.8.5.jar --env PIO_STORAGE_SOURCES_HBASE_TYPE=hbase,PIO_ENV_LOADED=1,PIO_STORAGE_SOURCES_HBASE_HOSTS=0,PIO_STORAGE_REPOSITORIES_METADATA_NAME=predictionio_metadata,PIO_FS_BASEDIR=/home/sahil/.pio_store,PIO_STORAGE_SOURCES_ELASTICSEARCH_HOSTS=localhost,PIO_HOME=/home/sahil/PredictionIO,PIO_FS_ENGINESDIR=/home/sahil/.pio_store/engines,PIO_STORAGE_SOURCES_HBASE_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_TYPE=elasticsearch,PIO_STORAGE_REPOSITORIES_METADATA_SOURCE=ELASTICSEARCH,PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=LOCALFS,PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=predictionio_eventdata,PIO_FS_TMPDIR=/home/sahil/.pio_store/tmp,PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_,PIO_STORAGE_SOURCES_LOCALFS_HOSTS=/home/sahil/.pio_store/models,PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE=HBASE,PIO_STORAGE_SOURCES_LOCALFS_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_PORTS=9300,PIO_STORAGE_SOURCES_LOCALFS_TYPE=localfs --engineId dem8XD7VweLWk5XAVX0as3J7APzTmnWw --engineVersion 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 --engineVariant /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/engine.json --verbosity 0 --jsonBasePath params
Spark assembly has been built with Hive, including Datanucleus jars on classpath
2015-02-17 16:29:56,568 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-17 16:29:57,362 INFO  workflow.CreateWorkflow$ - Extracting datasource params...
2015-02-17 16:29:57,433 INFO  workflow.CreateWorkflow$ - No 'name' is found. Default empty String wil be used.
2015-02-17 16:29:57,444 INFO  workflow.CreateWorkflow$ - datasource: (,DataSourceParams(9))
2015-02-17 16:29:57,445 INFO  workflow.CreateWorkflow$ - Extracting preparator params...
2015-02-17 16:29:57,447 INFO  workflow.CreateWorkflow$ - preparator: (,Empty)
2015-02-17 16:29:57,452 INFO  workflow.CreateWorkflow$ - Extracting serving params...
2015-02-17 16:29:57,454 INFO  workflow.CreateWorkflow$ - serving: (,Empty)
2015-02-17 16:29:58,286 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run
2015-02-17 16:29:58,287 INFO  workflow.CoreWorkflow$ - Start spark context
2015-02-17 16:30:05,874 INFO  workflow.CoreWorkflow$ - Data sanity checking is on.
2015-02-17 16:30:05,874 INFO  workflow.CoreWorkflow$ - Data Source
Gathering data from event server.
2015-02-17 16:30:12,275 INFO  workflow.CoreWorkflow$ - Number of training set: 1
2015-02-17 16:30:12,276 INFO  workflow.CoreWorkflow$ - Performing data sanity check on training data.
2015-02-17 16:30:12,277 INFO  workflow.CoreWorkflow$ - org.template.vanilla.TrainingData does not support data sanity check. Skipping check.
2015-02-17 16:30:12,277 INFO  workflow.CoreWorkflow$ - Data source complete
2015-02-17 16:30:12,278 INFO  workflow.CoreWorkflow$ - Preparator
2015-02-17 16:30:12,279 INFO  workflow.CoreWorkflow$ - Performing data sanity check on prepared data.
2015-02-17 16:30:12,279 INFO  workflow.CoreWorkflow$ - org.template.vanilla.PreparedData does not support data sanity check. Skipping check.
2015-02-17 16:30:12,280 INFO  workflow.CoreWorkflow$ - Preparator complete
2015-02-17 16:30:12,280 INFO  workflow.CoreWorkflow$ - Algo model construction
Running the K-Means clustering algorithm.
2015-02-17 16:30:30,746 INFO  workflow.CoreWorkflow$ - Performing data sanity check on model data.
2015-02-17 16:30:30,749 INFO  workflow.CoreWorkflow$ - org.apache.spark.rdd.ParallelCollectionRDD does not support data sanity check. Skipping check.
2015-02-17 16:30:30,750 INFO  workflow.CoreWorkflow$ - Evaluator is null. Stop here
2015-02-17 16:30:31,155 INFO  workflow.CoreWorkflow$ - Saved engine instance with ID: h7BaP5__R2Gp5NjLawm2GQ
2015-02-17 16:30:31,156 INFO  workflow.CoreWorkflow$ - Stop spark context
2015-02-17 16:30:32,366 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run completed.
2015-02-17 16:30:32,367 INFO  workflow.CoreWorkflow$ - Your engine has been trained successfully.
sahil@Sahil:~/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified$ 
sahil@Sahil:~/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified$ pio build; pio train;2015-02-17 16:48:44,402 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:48:44,404 INFO  tools.Console$ - Development tree detected. Building built-in engines.
2015-02-17 16:48:44,407 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at /home/sahil/PredictionIO to build.
2015-02-17 16:48:44,407 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:48:44,414 INFO  tools.Console$ - Going to run: [/home/sahil/PredictionIO/sbt/sbt, engines/publishLocal, engines/assemblyPackageDependency]
2015-02-17 16:48:59,620 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:48:59,622 INFO  tools.Console$ - Using command '/home/sahil/PredictionIO/sbt/sbt' at the current working directory to build.
2015-02-17 16:48:59,622 INFO  tools.Console$ - If the path above is incorrect, this process will fail.
2015-02-17 16:48:59,624 INFO  tools.Console$ - Uber JAR disabled. Making sure lib/pio-assembly-0.8.5.jar is absent.
2015-02-17 16:48:59,625 INFO  tools.Console$ - Going to run: /home/sahil/PredictionIO/sbt/sbt  package assemblyPackageDependency
2015-02-17 16:49:06,216 INFO  tools.Console$ - Build finished successfully.
2015-02-17 16:49:06,217 INFO  tools.Console$ - Looking for an engine...
2015-02-17 16:49:06,232 INFO  tools.Console$ - Found template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:49:06,232 INFO  tools.Console$ - Found template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:49:06,244 INFO  tools.Console$ - HADOOP_CONF_DIR is not set. Assuming HDFS is unavailable.
2015-02-17 16:49:07,837 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar
2015-02-17 16:49:07,898 INFO  tools.RegisterEngine$ - Copying file:/home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/target/scala-2.10/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar to file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar
2015-02-17 16:49:07,974 INFO  tools.RegisterEngine$ - Registering engine dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91
2015-02-17 16:49:07,991 INFO  tools.Console$ - Your engine is ready for training.
2015-02-17 16:49:08,956 INFO  tools.Console$ - Using existing engine manifest JSON at /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/manifest.json
2015-02-17 16:49:10,554 INFO  tools.RunWorkflow$ - Submission command: /home/sahil/PredictionIO/spark-1.2.0-bin-hadoop2.4/bin/spark-submit --class io.prediction.workflow.CreateWorkflow --name PredictionIO Training: dem8XD7VweLWk5XAVX0as3J7APzTmnWw 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 () --jars file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla_2.10-0.1-SNAPSHOT.jar,file:/home/sahil/.pio_store/engines/dem8XD7VweLWk5XAVX0as3J7APzTmnWw/85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91/template-scala-parallel-vanilla-assembly-0.1-SNAPSHOT-deps.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines_2.10-0.8.5.jar,/home/sahil/PredictionIO/engines/target/scala-2.10/engines-assembly-0.8.5-deps.jar --files /home/sahil/PredictionIO/conf/hbase-site.xml --driver-class-path /home/sahil/PredictionIO/conf /home/sahil/PredictionIO/assembly/pio-assembly-0.8.5.jar --env PIO_STORAGE_SOURCES_HBASE_TYPE=hbase,PIO_ENV_LOADED=1,PIO_STORAGE_SOURCES_HBASE_HOSTS=0,PIO_STORAGE_REPOSITORIES_METADATA_NAME=predictionio_metadata,PIO_FS_BASEDIR=/home/sahil/.pio_store,PIO_STORAGE_SOURCES_ELASTICSEARCH_HOSTS=localhost,PIO_HOME=/home/sahil/PredictionIO,PIO_FS_ENGINESDIR=/home/sahil/.pio_store/engines,PIO_STORAGE_SOURCES_HBASE_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_TYPE=elasticsearch,PIO_STORAGE_REPOSITORIES_METADATA_SOURCE=ELASTICSEARCH,PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=LOCALFS,PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=predictionio_eventdata,PIO_FS_TMPDIR=/home/sahil/.pio_store/tmp,PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_,PIO_STORAGE_SOURCES_LOCALFS_HOSTS=/home/sahil/.pio_store/models,PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE=HBASE,PIO_STORAGE_SOURCES_LOCALFS_PORTS=0,PIO_STORAGE_SOURCES_ELASTICSEARCH_PORTS=9300,PIO_STORAGE_SOURCES_LOCALFS_TYPE=localfs --engineId dem8XD7VweLWk5XAVX0as3J7APzTmnWw --engineVersion 85c8cc8d2027e7e5d4f24aa5b5f75e3002207c91 --engineVariant /home/sahil/PredictionIO/SahilCode/predictionio-template-scala-parallel-vanilla-modified/engine.json --verbosity 0 --jsonBasePath params
Spark assembly has been built with Hive, including Datanucleus jars on classpath
2015-02-17 16:49:11,860 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-17 16:49:12,663 INFO  workflow.CreateWorkflow$ - Extracting datasource params...
2015-02-17 16:49:12,735 INFO  workflow.CreateWorkflow$ - No 'name' is found. Default empty String wil be used.
2015-02-17 16:49:12,747 INFO  workflow.CreateWorkflow$ - datasource: (,DataSourceParams(9))
2015-02-17 16:49:12,747 INFO  workflow.CreateWorkflow$ - Extracting preparator params...
2015-02-17 16:49:12,749 INFO  workflow.CreateWorkflow$ - preparator: (,Empty)
2015-02-17 16:49:12,755 INFO  workflow.CreateWorkflow$ - Extracting serving params...
2015-02-17 16:49:12,756 INFO  workflow.CreateWorkflow$ - serving: (,Empty)
2015-02-17 16:49:13,605 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run
2015-02-17 16:49:13,606 INFO  workflow.CoreWorkflow$ - Start spark context
2015-02-17 16:49:24,803 INFO  workflow.CoreWorkflow$ - Data sanity checking is on.
2015-02-17 16:49:24,803 INFO  workflow.CoreWorkflow$ - Data Source
Gathering data from event server.
2015-02-17 16:49:30,134 INFO  workflow.CoreWorkflow$ - Number of training set: 1
2015-02-17 16:49:30,135 INFO  workflow.CoreWorkflow$ - Performing data sanity check on training data.
2015-02-17 16:49:30,136 INFO  workflow.CoreWorkflow$ - org.template.vanilla.TrainingData does not support data sanity check. Skipping check.
2015-02-17 16:49:30,136 INFO  workflow.CoreWorkflow$ - Data source complete
2015-02-17 16:49:30,136 INFO  workflow.CoreWorkflow$ - Preparator
2015-02-17 16:49:30,137 INFO  workflow.CoreWorkflow$ - Performing data sanity check on prepared data.
2015-02-17 16:49:30,138 INFO  workflow.CoreWorkflow$ - org.template.vanilla.PreparedData does not support data sanity check. Skipping check.
2015-02-17 16:49:30,138 INFO  workflow.CoreWorkflow$ - Preparator complete
2015-02-17 16:49:30,138 INFO  workflow.CoreWorkflow$ - Algo model construction
Running the K-Means clustering algorithm.
2015-02-17 16:49:46,295 INFO  workflow.CoreWorkflow$ - Performing data sanity check on model data.
2015-02-17 16:49:46,297 INFO  workflow.CoreWorkflow$ - org.apache.spark.rdd.ParallelCollectionRDD does not support data sanity check. Skipping check.
2015-02-17 16:49:46,298 INFO  workflow.CoreWorkflow$ - Evaluator is null. Stop here
2015-02-17 16:49:46,687 INFO  workflow.CoreWorkflow$ - Saved engine instance with ID: mKwLA7DARNG5OEXC9IGb9Q
2015-02-17 16:49:46,688 INFO  workflow.CoreWorkflow$ - Stop spark context
2015-02-17 16:49:47,875 INFO  workflow.CoreWorkflow$ - CoreWorkflow.run completed.
2015-02-17 16:49:47,875 INFO  workflow.CoreWorkflow$ - Your engine has been trained successfully.

